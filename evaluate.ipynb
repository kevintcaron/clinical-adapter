{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import evaluate\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred, task_name):\n",
    "    # print(eval_pred)\n",
    "    logits, labels = eval_pred\n",
    "    if task_name == 'ast':\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      accuracy = accuracy_score(labels, predictions)\n",
    "      precision = precision_score(labels, predictions, average='weighted')\n",
    "      recall = recall_score(labels, predictions, average='weighted')\n",
    "      f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "      return {\n",
    "          'accuracy': accuracy,\n",
    "          'precision': precision,\n",
    "          'recall': recall,\n",
    "          'f1': f1,\n",
    "      }\n",
    "    elif task_name == 'ner':\n",
    "      seqeval = evaluate.load(\"seqeval\")\n",
    "      predictions = np.argmax(logits, axis=2)\n",
    "      label_list  = [\"O\", \"B-test\", \"I-test\", \"B-problem\", \"I-problem\", \"B-treatment\", \"I-treatment\"]\n",
    "      true_predictions = [\n",
    "          [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "          for prediction, label in zip(predictions, labels)\n",
    "      ]\n",
    "      true_labels = [\n",
    "          [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "          for prediction, label in zip(predictions, labels)\n",
    "      ]\n",
    "\n",
    "      results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "      return {\n",
    "          \"accuracy\": results[\"overall_accuracy\"],\n",
    "          \"precision\": results[\"overall_precision\"],\n",
    "          \"recall\": results[\"overall_recall\"],\n",
    "          \"f1\": results[\"overall_f1\"]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT and add adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of trainable parameters: 133818432\n",
      "\n",
      "['ABSENT', 'POSSIBLE', 'PRESENT']\n",
      "{1: 'ABSENT', 2: 'POSSIBLE', 0: 'PRESENT'}\n"
     ]
    }
   ],
   "source": [
    "import adapters.composition as ac\n",
    "from transformers import AutoConfig\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "pretrained_model_name_or_path = \"bert-base-uncased\"\n",
    "#pretrained_model_name_or_path = \"emilyalsentzer/Bio_Discharge_Summary_BERT\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, model_max_length=150)\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"[entity]\"]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict,False)\n",
    "model = AutoAdapterModel.from_pretrained(pretrained_model_name_or_path = pretrained_model_name_or_path,\n",
    "                                         num_labels=3 , id2label = {0: 'PRESENT', 1: 'ABSENT', 2:'POSSIBLE'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "ast = model.load_adapter(\"./adapter_ast_SeqBnConfig_bert/\",with_head=True)\n",
    "#ast = model.load_adapter(\"./adapter_ast_pfeiffer_clinicalbert\",with_head=True)\n",
    "model.active_adapters = ast\n",
    "num_params = sum(p.numel() for p in model.base_model.parameters() if p.requires_grad )\n",
    "print()\n",
    "print(f\"Number of trainable parameters: {num_params}\")\n",
    "print()\n",
    "# How you can acces the labels and the mapping for a pretrained head\n",
    "print(model.get_labels())\n",
    "print(model.get_labels_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "number of beth training records: 73\n",
      "number of partners training records: 97\n",
      "number of all test records: 256\n",
      "total number of all combined records: 426\n",
      "\n",
      "no labels for record-58\n",
      "no labels for 262182942\n",
      "no labels for 0305\n",
      "\n",
      "number of beth records with labels: 72\n",
      "number of partners records with labels: 96\n",
      "number of test records with labels: 255\n",
      "total number of all combined records with labels: 423\n",
      "\n",
      "number of beth_and_partners examples: 6529\n",
      "number of test examples: 11868\n",
      "total number of combined examples: 18397\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365e07e12c704aa898c5905725fb50e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ff3e7834b54c0995d1f5dda6d2a484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.07683995366096497,\n",
       " 'test_accuracy': 0.9851432072292847,\n",
       " 'test_precision': 0.984978346928311,\n",
       " 'test_recall': 0.9851432072292847,\n",
       " 'test_f1': 0.9849269848903314,\n",
       " 'test_runtime': 100.0449,\n",
       " 'test_samples_per_second': 65.261,\n",
       " 'test_steps_per_second': 8.166}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from utils import AssertionDatai2b2, ConceptDatai2b2\n",
    "from datasets import Dataset, DatasetDict\n",
    "from adapters import AdapterTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "preprocessed_data_path = \"Data/preprocessed-data\"\n",
    "train_data_path = \"Data/concept_assertion_relation_training_data\"\n",
    "reference_test_data_path = \"Data/reference_standard_for_test_data\"\n",
    "test_data_path = \"Data/test_data\"\n",
    "ast_i2b2 = AssertionDatai2b2(preprocessed_data_path=preprocessed_data_path,\n",
    "                                 train_data_path=train_data_path,\n",
    "                                 reference_test_data_path=reference_test_data_path,\n",
    "                                 test_data_path=test_data_path)\n",
    "beth_and_partners_data, test_data, all_data = ast_i2b2.load_assertion_i2b2_data()\n",
    "# https://gist.github.com/vincenttzc/ceaa4aca25e53cb8da195f07e7d0af92\n",
    "\n",
    "\n",
    "def tokenize_function_ast(example):\n",
    "    return tokenizer(example[\"new_line\"],   padding=\"max_length\", truncation=True)\n",
    "lbl2id ={'absent': 1 ,'possible': 2, 'present':0}\n",
    "beth_and_partners_data['label_ids']= beth_and_partners_data.apply(lambda x: lbl2id[x['label']],axis=1)\n",
    "ds_train = Dataset.from_pandas(beth_and_partners_data[['label_ids','new_line']])\n",
    "\n",
    "tokenized_ds_train = ds_train.map(tokenize_function_ast)\n",
    "tokenized_ds_train.set_format(\"torch\")\n",
    "\n",
    "trainer = AdapterTrainer(model,compute_metrics=lambda p: compute_metrics(p, task_name='ast'))\n",
    "outputs = trainer.predict(tokenized_ds_train)\n",
    "outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca335c9a950f42c28e8cc2fa8b32f06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc107408cac41af8abf2b7e33b8fcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.29043903946876526,\n",
       " 'test_accuracy': 0.9463262554769127,\n",
       " 'test_precision': 0.944097160699813,\n",
       " 'test_recall': 0.9463262554769127,\n",
       " 'test_f1': 0.9438840273670824,\n",
       " 'test_runtime': 201.9122,\n",
       " 'test_samples_per_second': 58.778,\n",
       " 'test_steps_per_second': 7.35}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['label_ids']= test_data.apply(lambda x: lbl2id[x['label']],axis=1)\n",
    "ds_test = Dataset.from_pandas(test_data[['label_ids','new_line']])\n",
    "\n",
    "tokenized_ds_test = ds_test.map(tokenize_function_ast)\n",
    "tokenized_ds_test.set_format(\"torch\")\n",
    "\n",
    "trainer = AdapterTrainer(model,compute_metrics=lambda p: compute_metrics(p, task_name='ast'))\n",
    "outputs = trainer.predict(tokenized_ds_test)\n",
    "outputs.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test single example prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSENT\n",
      "ABSENT\n",
      "PRESENT\n",
      "ABSENT\n",
      "PRESENT\n",
      "ABSENT\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'PRESENT', 1: 'ABSENT', 2:'POSSIBLE'}\n",
    "sentence = [ \"Patient denies [entity] SOB [entity]\",\n",
    "            \"Patient do not have [entity] fever [entity]\",\n",
    "            \"had [entity] abnormal ett [entity] and referred for cath\",\n",
    "            \"The patient recovered during the night and now denies any [entity] shortness of breath [entity].\",\n",
    "         \"Patient with [entity] severe fever [entity].\",\n",
    "         \"Patient should abstain from [entity] painkillers [entity]\"]\n",
    "model.to('cpu')\n",
    "for s in sentence :\n",
    "  tokenized_input = tokenizer(s, return_tensors=\"pt\", padding=True)\n",
    "  outputs = model(**tokenized_input)\n",
    "  predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "  print(id2label[predicted_labels.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
